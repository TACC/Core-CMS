{% extends "guide.html" %}

{% block guide %}
  <h1>Portal Technology Stack</h1>

  <h2>Portal Overview</h2>

  <p>The TACC CEP, short for Core Experience Portal, is the core engine for all TACC portal projects. The primary objective for the Core Experience Portal is to establish and grow a codebase that can be used as a common baseline for all future portal projects undertaken at TACC. By cultivating this common codebase for all of our portal projects, we maintain better alignment between the core capabilities and technologies supported across all of the TACC portals. While some projects do have unique requirements, CEP provides an "out-of-the-box" framework for rapidly deploying a ready-to-use project infrastructure that includes all the common TACC portal capabilities pre-configured and in compliance with TACC's current best practices.</p>

  <h3>Common Portal Capabilities</h3>

  <ul>
    <li><strong>Dedicated VM</strong> resources for each portal (varies by portal requirements).</li>
    <li>An <strong>Isolated Tenant</strong> for seamless integration of the portal into the TACC ecosystem.</li>
    <li>A <strong>CMS</strong> for adding and editing content within static portal pages.</li>
    <li>Integrated <strong>Search</strong> capability using a variety of available technologies.</li>
    <li>An <strong>Issue Tracking &amp; Ticketing System</strong> integrated into the TACC RT ticketing system.</li>
    <li><strong>User Authentication</strong> and onboarding controls for portal access (requires a TACC account).</li>
    <li>A user <strong>Dashboard</strong> to monitor project activity and resource utilization.</li>
    <li>A shared <strong>Community Data</strong> storage system for collaborative data access.</li>
    <li>A <strong>Private Data</strong> storage system for an individual user's data.</li>
    <li><strong>Execution Systems</strong> for running applications on TACC resources.</li>
    <li>An <strong>Application Launcher</strong> for accessing public and private applications.</li>
    <li>A shared <strong>My Projects</strong> storage system for non-public group projects.</li>
    <li>A <strong>Public Data</strong> storage area for unauthenticated access to published work or data sets.</li>
  </ul>

  <h3>CEP Major Components</h3>

  <ul>
    <li>Content Management</li>
    <li>My Account</li>
    <li>My Dashboard</li>
    <li>Data Files (Data Depot)</li>
    <li>Applications (workspace)</li>
    <li>Search</li>
    <li>Notifications</li>
    <li>Allocations</li>
    <li>Jobs History</li>
    <li>Systems Status</li>
    <li>Portal Documentation & Guides</li>
  </ul>

  <p><em>Note: Any additional portal capabilities required by a project need to be identified and planned for independently.</em></p>

  <h2>High-level Architecture</h2>

  <p>The portal architecture operates in a tiered structure. Listed below in order from the outermost-tier and going inward, they are:</p>

  <dl class="s-inline-dl">
    <dt>Layer 4</dt>
    <dd>The user-facing web portal that enables users to interact with TAPIS through a browser-based GUI.</dd>

    <dt>Layer 3</dt>
    <dd>The TAPIS API that exposes access to Layer 2 and Layer 1 resources over a RESTful API as well as via a CLI for programmatic interaction with resources.</dd>

    <dt>Layer 2</dt>
    <dd>The middleware service that enables data management, job creation, and job scheduling (eg. Slurm, Kubernetes) across all layer 1 resources.</dd>

    <dt>Layer 1</dt>
    <dd>The physical infrastructure (storage, HPC and cloud systems) where data and applications are stored, manipulated, and executed (Corral, Frontera, Stampede2, Lonestar6, etc.).</dd>
  </dl>

  {% comment %} Need an updated diagram for portal architecture still. {% endcomment %}
  {% comment %} <p>
    <img src="https://cep-web-staging-01.tacc.utexas.edu/media/filer_public_thumbnails/filer_public/4e/de/4ede0716-07fb-455c-a3a4-ef0cc8b36857/image2018-12-13_15-26-55.png__400x225_q85_crop_subsampling-2_upscale.png" alt="Portal high-level architecture diagram as a triangle pointing up and segmented into four layers, where the top is layer 4 and the bottom is layer 1">
  </p> {% endcomment %}

  <h2>Backend (Server-side)</h2>

  <dl class="s-inline-dl">
    <dt>TAPIS</dt>
    <dd> The <a href="//tapis-project.org/" rel="nofollow" target="_blank">TAPIS Project</a> is an open source, science-as-a-service API platform that provides HPC and file management integration. TAPIS, short for TACC APIs, is developed and maintained by TACC.</dd>

    <dt>HPC Connectivity</dt>
    <dd>From within the portal access to HPC systems is primarily through TAPIS. TAPIS calls in turn submit jobs through Slurm. Multiple platforms are used based on the particular application (or based on availability) -- Frontera, Lonestar6 and Stampede2 are the primary target platforms for application deployments.</dd>

    <dt>File Storage</dt>
    <dd>Files are stored on Corral, a mirrored GPFS storage facility, and backed up to Ranch, TACC's long term tape-based storage system. From the web interface, all file I/O is done through TAPIS calls, to maintain consistent metadata. Users also have the option to upload through Globus online, or through public cloud storage facilities like Dropbox, Box, Google Drive. Cloud-based data imports are called through TAPIS in order to maintain consistent metadata within the TACC ecosystem.</dd>

    <dt>Applications</dt>
    <dd>Portal applications have three distinct components. The applications themselves are installed on the execution systems (the HPC platforms) or in containers that run on virtual infrastructure. Applications are then registered with the TAPIS API where they are defined as TAPIS application records, pointing to a zip file on Corral collocated with the application's associated metadata records. a JSON document for each application defines the UI exposed in the Portal. Runtime instruction inputs are supplied by the jobs created by portal users through TAPIS. TAPIS also supplies callbacks for updates on job statuses.</dd>

    <dt>API</dt>
    <dd>The API manager (APIM) sits in front of TAPIS core and handles user authentication/authorization, proxying/routing, client management, analytics, rate limiting, and many other features. It provides a unified namespace for the entire API to be hosted under.</dd>

    <dt>Projects API</dt>
    <dd>This API returns project listings and associated files utilizing the Django framework.</dd>
  </dl>

  <h2>Frontend (Client-side)</h2>

  <h3>Web Portal</h3>

  <p>A web-based portal using TAPIS to manage apps, data storage, reconfigurable workflows, and HPC resource interactions. The architecture for the web portal provides data management, analysis, and simulation capabilities to users through a unified web interface. The dashboard provides an overview of jobs, data storage, allocations, and system resource status. Users will primarily interact with the portal under the <em>My Account</em> view or the <em>My Dashboard</em> view. The portal dashboard includes dedicated interfaces for <em>Data Files</em>, <em>Applications</em>, <em>Allocations</em>, <em>History</em>, and <em>System Status</em>.</p>

  <h3>My Dashboard</h3>

  <p>The dashboard displays availability of TACC resources, and user allocation usages. The CEP infrastructure runs on TACC hosted Virtual Machines, Django/Angular web portal with responsive layout. Every Portal project will have a dedicated VM resources. system status instrumentation will be provided via APIs. The AngularJS framework works by first reading the HTML page, which has additional custom tag attributes embedded into it. Angular interprets those attributes as directives to bind input or output parts of the page to a model that is represented by standard JavaScript variables. The values of those JavaScript variables can be manually set within the code, or retrieved from static or dynamic JSON resources.</p>

  <h3>Data Files</h3>

  <p>Data Files is a collection of storage spaces where user and project data are located, stored, and ultimately organized by users to curate publications and share information. Data Repository is the place where experimental and simulation results are stored for long term. Working storage is the area to share and collaborate with data that is not yet published, Workspace to allow for the analysis of data, a gateway to large scale HPC resources and simulation tools. The data is organized in three categories:</p>

  <ul>
    <li><strong>My Data</strong> drive hosts any data the user decides to store there. This is also the default storage device for application output data from the Workspace. Within the MyData space, users can organize, manipulate, view, copy the data to an external storage resource.</li>
    <li><strong>My Projects</strong> view displays the projects the user has created or has been shared with by another team member. Within this scope, files associated with a specific project can be manipulated, tagged, organized, and published through the Data Curation workflow, where publications automatically receive a DOI.</li>
    <li><strong>Community Data</strong> view displays non-curated user-contributed data for public use.</li>
  </ul>

  <h3>Applications</h3>

  <p>APPS are executable code available for invocation through Agaveâ€™s Jobs service on a specific execution system. If a single code needs to be run on multiple systems, each combination and system needs to be defined as an app. Code that can be forked at the command line or submitted to a batch scheduler can be registered as an Agave app and run through the Jobs service. The user sends a request by filling in the application inputs and job details on CEP portal. The app is packaged with 3 files:</p>

  <dl class="s-inline-dl  u-nested-text-content">
    <dt>App.json</dt><dd>JSON file with application definitions and parameters such as (name, version, label).</dd>
    <dt>Wrapper.sh</dt><dd>wrapper file, shell script which executes the job on HPC/Docker.</dd>
    <dt>Test.sh</dt><dd>runs a test before the job is launched and cleans up the files.</dd>
  </dl>

  <h3>Notifications</h3>

  <p>Notification Bell enables the user to view information and status of submitted jobs.</p>

  <h3>Search</h3>

  <p>CEP has a multi-tenant capable full-text search engine with an HTTP web interface and schema-free JSON documents based on ElasticSearch. Users can search for data files, or text.</p>

  <h2>Environment</h2>

  <p>The Core portal utilizes a wide variety of technologies developed by multiple technology vendors. Below is a list of the primary libraries, frameworks and APIs leveraged in the core portal tech stack.</p>

  <dl class="s-inline-dl">
    <dt>NginX</dt>
      <dd>When user sends http request to call the CEP URL, it goes to nginx webserver, a proxy, intermediary for requests from clients seeking resources from other&nbsp;servers. Nginx carry the request to the Web Server Gateway.</dd>

    <dt>WSGI</dt>
      <dd>Web Server Gateway Interface, a calling convention for a web hosting server to forward requests to web applications or frameworks written in the Python programming language. It forwards the CEP user request to Django.</dd>

    <dt>Django</dt>
      <dd>A backend server manages the requests and response cycle. Pass the CEP user request to Angular and the return back information (Python base)</dd>

    <dt>Angular</dt>
      <dd>The JavaScript framework taking care of all the frontend client-side requests, activities that taking place on that page, Requests method such as get, post, put.</dd>

    <dt>PostgreSQL</dt>
      <dd>The object-relational database management system</dd>

    <dt>Agave Platform</dt>
      <dd>is an open source, science-as-a-service API platform, securely manage, move, and share data metadata, shared high performance computing (HPC) Cloud, and Big Data resources under a single, web-friendly REST API. Run code, simplifies building web portals that use back-end computing, and run executable systems.</dd>

    <dt>RABBITMQ</dt>
      <dd>The portal Messages Broker, enterprise messaging system modeled on the Advanced Message Queuing Protocol (AMQP) standard. a message broker that acts as an intermediary platform to processing communication between two applications.</dd>

    <dt>CELERY</dt>
      <dd>A Distributed message passing queue.</dd>

    <dt>HPC</dt>
      <dd>High Performance Computers, TACC super computer resources (Frontera, Stampede, etc.)</dd>

    <dt>CMS</dt>
      <dd>The Content Management System used in the core portal.</dd>
  </dl>
{% endblock guide %}
