{% extends "guide.html" %}

{% block guide %}
  <h1>Portal Technology Stack</h1>

  <h2>Portal Overview</h2>

  <p>The TACC CEP, short for Core Experience Portal, is the core engine for all TACC portal projects. The primary objective for the Core Experience Portal is to establish and grow a codebase that can be used as a common baseline for all future portal projects undertaken at TACC. By cultivating this common codebase for all of our portal projects, we maintain better alignment between the core capabilities and technologies supported across all of the TACC portals. While some projects do have unique requirements, CEP provides an "out-of-the-box" framework for rapidly deploying a ready-to-use project infrastructure that includes all the common TACC portal capabilities pre-configured and in compliance with TACC's current best practices.</p>

  <h3>Common Portal Capabilities</h3>

  <ul>
    <li><strong>Dedicated VM</strong> resources for each portal (varies by portal requirements).</li>
    <li>An <strong>Isolated Tenant</strong> for seamless integration of the portal into the TACC ecosystem.</li>
    <li>A <strong>CMS</strong> for adding and editing content within static portal pages.</li>
    <li>Integrated <strong>Search</strong> capability using a variety of available technologies.</li>
    <li>An <strong>Issue Tracking &amp; Ticketing System</strong> integrated into the TACC RT ticketing system.</li>
    <li><strong>User Authentication</strong> and onboarding controls for portal access (requires a TACC account).</li>
    <li>A user <strong>Dashboard</strong> to monitor project activity and resource utilization.</li>
    <li>A shared <strong>Community Data</strong> storage system for collaborative data access.</li>
    <li>A <strong>Private Data</strong> storage system for an individual user's data.</li>
    <li><strong>Execution Systems</strong> for running applications on TACC resources.</li>
    <li>An <strong>Application Launcher</strong> for accessing public and private applications.</li>
    <li>A shared <strong>My Projects</strong> storage system for non-public group projects.</li>
    <li>A <strong>Public Data</strong> storage area for unauthenticated access to published work or data sets.</li>
  </ul>

  <h3>CEP Major Components</h3>

  <ul>
    <li>Content Management</li>
    <li>My Account</li>
    <li>My Dashboard</li>
    <li>Data Files (Data Depot)</li>
    <li>Applications</li>
    <li>Search</li>
    <li>Notifications</li>
    <li>Allocations</li>
    <li>Jobs History</li>
    <li>Systems Status</li>
    <li>Portal Documentation & Guides</li>
  </ul>

  <p><em>Note: Any additional portal capabilities required by a project need to be identified and planned for independently.</em></p>

  <h2>High-level Architecture</h2>

  <p>The portal architecture operates in a tiered structure. Listed below in order from the outermost-tier and going inward, they are:</p>

  <dl class="s-inline-dl">
    <dt>Layer 4</dt>
    <dd>The user-facing web portal that enables users to interact with TAPIS through a browser-based GUI.</dd>

    <dt>Layer 3</dt>
    <dd>The TAPIS API that exposes access to Layer 2 and Layer 1 resources over a RESTful API as well as via a CLI for programmatic interaction with resources.</dd>

    <dt>Layer 2</dt>
    <dd>The middleware service that enables data management, job creation, and job scheduling (e.g. Slurm, Kubernetes) across all Layer 1 resources.</dd>

    <dt>Layer 1</dt>
    <dd>The physical infrastructure (storage, HPC and cloud systems) where data and applications are stored, manipulated, and executed (Corral, Frontera, Stampede2, Lonestar6, etc.).</dd>
  </dl>

  {% comment %} Need an updated diagram for portal architecture still. {% endcomment %}
  {% comment %} <p>
    <img src="https://cep-web-staging-01.tacc.utexas.edu/media/filer_public_thumbnails/filer_public/4e/de/4ede0716-07fb-455c-a3a4-ef0cc8b36857/image2018-12-13_15-26-55.png__400x225_q85_crop_subsampling-2_upscale.png" alt="Portal high-level architecture diagram as a triangle pointing up and segmented into four layers, where the top is layer 4 and the bottom is layer 1">
  </p> {% endcomment %}

  <h2>Backend (Server-side)</h2>

  <dl class="s-inline-dl">
    <dt>TAPIS</dt>
    <dd> The <a href="//tapis-project.org/" rel="nofollow" target="_blank">TAPIS Project</a> is an open source, science-as-a-service API platform that provides HPC and file management integration. TAPIS, short for TACC APIs, is developed and maintained by TACC.</dd>

    <dt>HPC Connectivity</dt>
    <dd>From within the portal access to HPC systems is primarily through TAPIS. TAPIS calls in turn submit jobs through Slurm. Multiple platforms are used based on the particular application (or based on availability) -- Frontera, Lonestar6 and Stampede2 are the primary target platforms for application deployments.</dd>

    <dt>File Storage</dt>
    <dd>Files are stored on Corral, a mirrored GPFS storage facility, and backed up to Ranch, TACC's long term tape-based storage system. From the web interface, all file I/O is done through TAPIS calls, to maintain consistent metadata. Users also have the option to upload through Globus online, or through public cloud storage facilities like Dropbox, Box, Google Drive. Cloud-based data imports are called through TAPIS in order to maintain consistent metadata within the TACC ecosystem.</dd>

    <dt>Applications</dt>
    <dd>Portal applications are installed on the execution systems (the HPC platforms) or in containers that run on virtual infrastructure. Applications are then registered with the TAPIS API where they are defined as TAPIS application records, pointing to a zip file on Corral collocated with the application's associated metadata records. A JSON document for each application defines the UI exposed in the Portal. Runtime instruction inputs are supplied by the jobs created by portal users through TAPIS. TAPIS also supplies callbacks for updates on job statuses.</dd>

    <dt>API</dt>
    <dd>The API manager (APIM) sits in front of TAPIS core and handles user authentication/authorization, proxying/routing, client management, analytics, rate limiting, and many other features. It provides a unified namespace for the entire API to be hosted under.</dd>

    <dt>Projects API</dt>
    <dd>This API returns project listings and associated files utilizing the Django framework.</dd>
  </dl>

  <h2>Frontend (Client-side)</h2>

  <dl class="s-inline-dl">
  <dt>Web Portal</dt>
  <dd>A web-based portal using TAPIS to manage apps, data storage, reconfigurable workflows, and HPC resource interactions. The architecture for the web portal provides data management, analysis, and simulation capabilities to users through a unified web interface. The dashboard provides an overview of jobs, data storage, allocations, and system resource status. Users will primarily interact with the portal under the <em>My Account</em> view or the <em>My Dashboard</em> view, which includes dedicated interfaces for <em>Data Files</em>, <em>Applications</em>, <em>Allocations</em>, <em>History</em>, and <em>System Status</em>.</dd>

  <dt>My Account</dt>
  <dd>This view provides information about the user's profile, password management, activated licenses for applications, and integrated third-party applications/resources.</dd>

  <dt>My Dashboard</dt>
  <dd>The dashboard presents users with a unified interface to enable their research.</dd>

  <dl class="s-inline-dl  u-nested-text-content">
    <dt>Data Files</dt><dd>This view is where users can manage their data and shared data resources.</dd>
    <dt>Applications</dt><dd>This view allows users to select and execute various applications (both remote execution and interactive jobs).</dd>
    <dt>Allocations</dt><dd>This view allows users to view and request available allocation resources for their projects as well as monitor who has access to said resources.</dd>
    <dt>History</dt><dd>This view logs all jobs the user has executed (or is currently executing) with options to view the job details and output location.</dd>
    <dt>System Status</dt><dd>This view displays detailed operational status information about the available HPC systems, including queue states, jobs currently running/waiting in queue, and the number of idle nodes on the resource.</dd>
  </dl>

  <dt>Data Files</dt>
  <dd>Data Files is a collection of storage spaces where user and project data are located, stored, and organized by users to curate publications and share information. These storage systems are integrated into the portal such that users can store research and experimental or collected data for ongoing projects, share their data with other portal users to foster collaboration on unpublished research, use their data as inputs for scheduled jobs or interactive analysis, and in some cases share the findings with the public. There are five categories of storage system in the portal:</dd>

  <dl class="s-inline-dl  u-nested-text-content">
    <dt>My Data</dt><dd>These systems can exist on Work or Scratch. These systems host data that is only accessible to the authenticated portal user. This is the default storage system for all application output data. Within the MyData space, users can organize, manipulate, view, and copy the data to other storage resources.</dd>
    <dt>Shared Workspaces</dt><dd>Displays any workspaces the user has created or that another user has shared with the user. Within this context, files associated with a specific project can be manipulated, tagged, organized, and published through the Data Curation workflow, where publications automatically receive a DOI.</dd>
    <dt>Community Data</dt><dd>Displays non-curated user-contributed data shared with all users who have access to the portal.</dd>
    <dt>Public Data</dt><dd>Exposes curated data for making research findings publicly available.</dd>
    <dt>Cloud storage</dt><dd>These systems expose third-party storage resources that have been integrated into the portal user's account (i.e. - Google Drive, Box, UT Box).</dd>
  </dl>

  <dt>Applications</dt>
  <dd>Applications are executable code available for invocation through the TAPIS Jobs service on a specific execution system. If a single code needs to be run on multiple systems, each combination and system needs to be defined as an app. Code that can be forked at the command line or submitted to a batch scheduler can be registered as a TAPIS app and run through the Jobs service. The user sends a request by populating the application inputs and job details in the portal. Applications are packed as three distinct components:</dd>

  <dl class="s-inline-dl  u-nested-text-content">
    <dt>App.json</dt><dd>A JSON file with application definitions and input parameters (i.e. name, version, label).</dd>
    <dt>Wrapper.sh</dt><dd>A shell script which executes the job on the targeted resource (i.e. an HPC system or virtualized container).</dd>
    <dt>Test.sh</dt><dd>A test script that is run before the job is launched to validate job inputs and parameters.</dd>
  </dl>

  <dt>Notifications</dt>
  <dd>The dashboard includes a notification icon (to the right of the History menu link) that indicates the number of unviewed notifications the use has. Selecting the history area enables the user to view the status and detailed information about all submitted jobs.</dd>

  <dt>Search</dt>
  <dd>CEP has a multi-tenant capable full-text search engine based on ElasticSearch and Google. Users can search for data files or text that appears on static content pages in the CMS or within the names of files that are stored on the available storage systems for the given user.</dd>

  <h2>Environment</h2>

  <p>The Core portal utilizes a wide variety of technologies developed by multiple technology vendors. Below is a list of the primary libraries, frameworks and APIs leveraged in the core portal tech stack.</p>

  <dl class="s-inline-dl">
    <dt>React</dt>
      <dd>The JavaScript framework providing all client-side functionality. This is the broswer-based client the user interacts with when accessing the portal.</dd>

    <dt>NginX</dt>
      <dd>The nginx webserver is as a proxy for all portal users, acting as intermediary for requests from clients seeking resources from the server. Nginx routes requests to the appropriate backend server.</dd>

    <dt>WSGI</dt>
    <dd>Web Server Gateway Interface, a calling convention for a web hosting server to forward requests to web applications or frameworks written in the Python programming language. It forwards the CEP user request to DjangoCMS and Django as appropriate.</dd>

    <dt>DjangoCMS</dt>
    <dd>The Content Management System used in the core portal architecture for all static web content and portal project branding.</dd>

    <dt>Django</dt>
    <dd>A backend server application framework that manages the requests and response cycle. Django provides all of the functionality (exposed via the React client) for interacting with the TACC ecosystem, TAPIS APIs and any integrated portal resources.</dd>

    <dt>PostgreSQL</dt>
    <dd>The object-relational database management system that supports all CEP portals.</dd>

    <dt>TAPIS Project</dt>
    <dd>TAPIS is an open source, science-as-a-service API platform, to securely manage data/metadata, and expose high performance computing (HPC) resources under a single, web-friendly REST API. The TAPIS API simplifies building web portals that use back-end computing and run remote jobs against execution systems.</dd>

    <dt>RabbitMQ</dt>
    <dd>The portal Message Broker, RabbitMQ provides enterprise messaging system modeled on the Advanced Message Queuing Protocol (AMQP) standard. It functions as a message broker that acts as an intermediary platform to processing communication between two applications (in this case between Django and Celery Workers, which execute all tasks asynchronously for the portal).</dd>

    <dt>Celery</dt>
    <dd>A Distributed message passing queue for distributing asynchronous processes across execution resources to enable non-blocking code execution in Django and support SPA-like behavior in the React client.</dd>

    <dt>HPC</dt>
    <dd>High Performance Computers are the primary resource available at TACC in the form of Frontera, Stampede2, and Longhorn6, to name only a few available HPC systems. Different projects target different HPC resources based on the kind of research and code execution workflows required by each individual project.</dd>
  </dl>
{% endblock guide %}
